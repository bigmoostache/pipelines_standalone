{"url":"https://lucario.croquo.com","project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","elements":{"0":{"file_id":5410,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"7ee1459a-47ce-4380-acde-a911eceddbed_20250220011643.pdf","file_name":"document.pdf","file_hash":"b74d2224865f6c2c4ee0d587aaf96f47b902e3a0256177515525f06e425e05d2","file_ext":"pdf","upload_date":"2025-02-20T01:16:44.050401","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Analog architectures for neural network acceleration based on non-volatile memory\",\n  \"reference\": \"T. P. Xiao, C. H. Bennett, B. Feinberg, S. Agarwal, & M. J. Marinella (2020). Analog architectures for neural network acceleration based on non-volatile memory. Applied Physics Reviews, 7(031301). https://doi.org/10.1063/1.5143815\",\n  \"doi\": \"10.1063/1.5143815\",\n  \"date\": \"09-07-2020\",\n  \"journal\": \"Applied Physics Reviews\",\n  \"authors\": [\n    \"T. Patrick Xiao\",\n    \"Christopher H. Bennett\",\n    \"Ben Feinberg\",\n    \"Sapan Agarwal\",\n    \"Matthew J. Marinella\"\n  ],\n  \"context\": \"The article discusses analog hardware accelerators for neural networks, focusing on leveraging non-volatile memory to improve performance and energy efficiency. It highlights challenges and design considerations specific to analog neuromorphic hardware for deep learning tasks.\",\n  \"other_metadata\": [\n    \"Keywords: analog accelerators, neural networks, non-volatile memory, neuromorphic hardware\",\n    \"Abstract: Reviews implementations of analog hardware accelerators for deep supervised learning, focusing on circuit and architecture design. Emphasizes challenges and trade-offs in design at different hierarchical levels.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"1":{"file_id":5411,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"ee491b90-eb6b-4d29-962b-18f0259d860a_20250220011658.pdf","file_name":"document.pdf","file_hash":"94cc982c37bb3bdd0fafc27c51dbeff94aa2e7ef16ebd1fec5a8e108271ae809","file_ext":"pdf","upload_date":"2025-02-20T01:16:58.635109","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Review of ASIC accelerators for deep neural network\",\n  \"reference\": \"Machupalli, R., Hossain, M., & Mandal, M. (2022). Review of ASIC accelerators for deep neural network. Microprocessors and Microsystems, https://doi.org/10.1016/j.micpro.2022.104441\",\n  \"doi\": \"10.1016/j.micpro.2022.104441\",\n  \"date\": \"12-01-2022\",\n  \"journal\": \"Microprocessors and Microsystems\",\n  \"authors\": [\n    \"Raju Machupalli\",\n    \"Masum Hossain\",\n    \"Mrinal Mandal\"\n  ],\n  \"context\": \"This article reviews and classifies existing ASIC accelerators for deep neural networks based on optimization techniques to identify the ideal accelerator model for specific applications. It identifies key areas like ALU, dataflow, and sparsity to enhance accelerator performance and discusses the trade-offs in optimizing hardware for different DNN types.\",\n  \"other_metadata\": [\n    \"Keywords: Deep neural network, Hardware accelerator, Neural processor, Domain specific accelerator, ASIC\",\n    \"Abstract: Deep neural networks (DNNs) require high computational power for complex applications, necessitating domain-specific accelerators. This paper reviews and categorizes existing DNN hardware accelerators, providing insights into their optimization techniques and performance parameters.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"2":{"file_id":5513,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"0282f4a2-7cfa-4912-820b-83386f2e6824_20250220011710.pdf","file_name":"document.pdf","file_hash":"350f8c4258bc23fa07071a458f49e589e541dde9613ca1b71d2ba9fa431857a3","file_ext":"pdf","upload_date":"2025-02-20T01:17:10.857554","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Brain Tumor Segmentation with Deep Neural Networks\",\n  \"reference\": \"Havaei, M., Davy, A., Warde-Farley, D., Biard, A., Courville, A., Bengio, Y., Pal, C., Jodoin, P.-M., & Larochelle, H. (2016). Brain tumor segmentation with deep neural networks. Medical Image Analysis, Elsevier B.V.\",\n  \"doi\": \"10.1016/j.media.2016.05.004\",\n  \"date\": \"19-05-2016\",\n  \"journal\": \"Medical Image Analysis\",\n  \"authors\": [\n    \"Mohammad Havaei\",\n    \"Axel Davy\",\n    \"David Warde-Farley\",\n    \"Antoine Biard\",\n    \"Aaron Courville\",\n    \"Yoshua Bengio\",\n    \"Chris Pal\",\n    \"Pierre-Marc Jodoin\",\n    \"Hugo Larochelle\"\n  ],\n  \"context\": \"This article presents a fully automatic brain tumor segmentation method based on deep neural networks tailored to segment glioblastomas in MRI images. The novel CNN architecture can efficiently identify both local and global features, offering superior speed and performance relative to current methods. Evaluations on the BRATS 2013 dataset showed the architecture improves over state-of-the-art while being more than 30 times faster.\",\n  \"other_metadata\": [\n    \"Keywords: Brain tumor segmentation, Deep neural networks, Convolutional neural networks, Cascaded convolutional neural networks\",\n    \"Received: 27 April 2015\",\n    \"Revised: 2 March 2016\",\n    \"Accepted: 11 May 2016\",\n    \"Impact: Improved diagnostics, growth rate prediction, and treatment planning for brain tumors\",\n    \"Abstract: Fully automatic segmentation utilizing a novel two-pathway CNN architecture and a cascaded approach for enhanced speed and accuracy.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"3":{"file_id":5527,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"d7d270fb-f7f5-4dfe-9680-d15b0c291f5a_20250220011723.pdf","file_name":"document.pdf","file_hash":"1c9a9fdd36b657ec7881000db4504308cde749754d57894430fb437a862e113f","file_ext":"pdf","upload_date":"2025-02-20T01:17:24.254120","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"A Survey of Accelerator Architectures for Deep Neural Networks\",\n  \"reference\": \"Chen, Yiran, Xie, Yuan, Song, Linghao, Chen, Fan, & Tang, Tianqi (2020). A Survey of Accelerator Architectures for Deep Neural Networks. Engineering, 6(2020), 264-274. https://doi.org/10.1016/j.eng.2020.01.007\",\n  \"doi\": \"10.1016/j.eng.2020.01.007\",\n  \"date\": \"29-01-2020\",\n  \"journal\": \"Engineering\",\n  \"authors\": [\n    \"Yiran Chen\",\n    \"Yuan Xie\",\n    \"Linghao Song\",\n    \"Fan Chen\",\n    \"Tianqi Tang\"\n  ],\n  \"context\": \"This article summarizes recent advances in accelerator designs for deep neural networks (DNNs), discussing various architectures supporting DNN execution, including emerging technologies like AI accelerators. It offers insights into the structural optimizations and the scalability for specific data processing challenges posed by machine learning growth.\",\n  \"other_metadata\": [\n    \"Keywords: Deep neural network, Domain-specific architecture, Accelerator\",\n    \"Open Access: CC BY-NC-ND license\",\n    \"Journal ISSN: 2095-8099\"\n  ]\n}","text":null,"score":null,"raw_url":null},"4":{"file_id":5600,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"33a6208d-1427-4258-aa74-08715d3180af_20250220011735.pdf","file_name":"document.pdf","file_hash":"7d952f254a93e46bb688cb153fe667e0298859fe33d295e6f760b3fba63225aa","file_ext":"pdf","upload_date":"2025-02-20T01:17:35.227234","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization, and Huffman Coding\",\n  \"reference\": \"Han, S., Mao, H., & Dally, W. J. (2016). Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization, and Huffman Coding. International Conference on Learning Representations (ICLR).\",\n  \"doi\": null,\n  \"date\": \"05-04-2016\",\n  \"journal\": \"International Conference on Learning Representations (ICLR)\",\n  \"authors\": [\n    \"Song Han\",\n    \"Huizi Mao\",\n    \"William J. Dally\"\n  ],\n  \"context\": \"The article introduces a method called 'deep compression', which combines network pruning, trained quantization, and Huffman coding to significantly decrease storage demands for deep neural networks without a loss in accuracy. This method reduces storage requirements for neural networks by up to 49\\u00d7, enabling deployment on devices with limited hardware resources.\",\n  \"other_metadata\": [\n    \"Keywords: neural networks, compression, pruning, quantization, Huffman coding\",\n    \"Objective: Enable efficient deployment of deep neural networks on resource-constrained devices without sacrificing accuracy.\",\n    \"Methodology: Combines pruning (removal of redundant connections), trained quantization (weight sharing), and Huffman coding into a compression pipeline.\",\n    \"Results: Achieved a 35\\u00d7 reduction in model size for AlexNet and a 49\\u00d7 reduction for VGG-16 without any loss of accuracy.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"5":{"file_id":5691,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"6a65efc7-fcc8-459a-98ba-fc424f28081f_20250220011745.pdf","file_name":"document.pdf","file_hash":"877b248c81ba3ebfea1d2a85b099adda0d6887703c5f05f8225296f9d1f86fd8","file_ext":"pdf","upload_date":"2025-02-20T01:17:45.641104","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\",\n  \"reference\": \"Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., & Bengio, Y. (2018). Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. Retrieved from http://jmlr.org/papers/v18/16-456.html\",\n  \"doi\": null,\n  \"date\": \"2018\",\n  \"journal\": \"Journal of Machine Learning Research\",\n  \"authors\": [\n    \"Itay Hubara\",\n    \"Matthieu Courbariaux\",\n    \"Daniel Soudry\",\n    \"Ran El-Yaniv\",\n    \"Yoshua Bengio\"\n  ],\n  \"context\": \"The study introduces a method to train quantized neural networks (QNNs) using low precision weights and activations, achieving comparable prediction accuracy to full precision networks. Experiments were conducted on datasets like MNIST, CIFAR-10, and ImageNet, showing effective performance with reduced resource consumption.\",\n  \"other_metadata\": [\n    \"Keywords: deep learning, neural networks compression, energy efficient neural networks, computer vision, language models\",\n    \"License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"6":{"file_id":5707,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"cb4b0d2d-b828-4b13-8a02-f9bb0d3034d8_20250220011756.pdf","file_name":"document.pdf","file_hash":"e246c4475ea15c8257ca9464c70cf7e1ac1d35c3989016d487d56c0cd9586a57","file_ext":"pdf","upload_date":"2025-02-20T01:17:56.743521","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Hardware-oriented Approximation of Convolutional Neural Networks\",\n  \"reference\": \"Gysel, P., Motamedi, M., & Ghiasi, S. (2016). Hardware-oriented approximation of convolutional neural networks. Department of Electrical and Computer Engineering, University of California, Davis.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Philipp Gysel\",\n    \"Mohammad Motamedi\",\n    \"Soheil Ghiasi\"\n  ],\n  \"context\": \"The article introduces Ristretto, a framework designed to reduce the computational requirements of Convolutional Neural Networks (CNNs) by converting models from floating point to fixed point, without significant loss in accuracy. This allows faster and more power-efficient model execution.\",\n  \"other_metadata\": [\n    \"Keywords: CNN, hardware acceleration, fixed point arithmetic, model compression, Ristretto\",\n    \"Ristretto achieved 8-bit model condensation for CaffeNet and SqueezeNet with a maximum error tolerance of 1%.\",\n    \"Discusses the implementation and advantages of dynamic fixed point representation.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"7":{"file_id":5753,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"7418cd95-d90a-429b-96a2-9564f806ed85_20250220011807.pdf","file_name":"document.pdf","file_hash":"0ed396945550d9063bf2e1fac686358f0cfbd792348db420020b47e7a783f8c3","file_ext":"pdf","upload_date":"2025-02-20T01:18:07.456880","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Training and Inference with Integers in Deep Neural Networks\",\n  \"reference\": \"Wu, S., Li, G., Chen, F., & Shi, L. (2023). Training and Inference with Integers in Deep Neural Networks. Center for Brain Inspired Computing Research, Tsinghua University.\",\n  \"doi\": null,\n  \"date\": \"2023\",\n  \"journal\": null,\n  \"authors\": [\n    \"Shuang Wu\",\n    \"Guoqi Li\",\n    \"Feng Chen\",\n    \"Luping Shi\"\n  ],\n  \"context\": \"The article discusses a new method called 'WAGE' that discretizes both training and inference processes in deep neural networks to low-bitwidth integers, presenting a framework that minimizes computational complexity and energy consumption.\",\n  \"other_metadata\": [\n    \"Keywords: Deep Learning, Integer Arithmetic, Low-Bitwidth Networks, Deep Neural Networks, Embedded Systems\",\n    \"Objectives: Develop a method to reduce precision in both training and inference for DNNs.\",\n    \"Methods: Introduced 'WAGE' strategy using linear mapping, batch normalization replacement, and orientation-preserved shifting.\",\n    \"Results: Improved accuracies demonstrated in multiple datasets with reduced computational complexity and energy efficiency.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"8":{"file_id":5819,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"1b116ceb-d7ac-44b9-b4a7-55bc485f41bc_20250220011818.pdf","file_name":"document.pdf","file_hash":"e45075eafe5f8397694a9851629c3d5a5750bbcf8d249a46ecce70c85f4e7702","file_ext":"pdf","upload_date":"2025-02-20T01:18:18.265255","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"ShiDianNao: Shifting Vision Processing Closer to the Sensor\",\n  \"reference\": \"Du, Z., Fasthuber, R., Chen, T., Ienne, P., Li, L., Luo, T., Feng, X., Chen, Y., & Temam, O. (2015). ShiDianNao: Shifting Vision Processing Closer to the Sensor. In Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA) 2015, Portland, OR, USA. ACM.\",\n  \"doi\": \"10.1145/2749469.2750389\",\n  \"date\": \"13-06-2015\",\n  \"journal\": \"Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA)\",\n  \"authors\": [\n    \"Zidong Du\",\n    \"Robert Fasthuber\",\n    \"Tianshi Chen\",\n    \"Paolo Ienne\",\n    \"Ling Li\",\n    \"Tao Luo\",\n    \"Xiaobing Feng\",\n    \"Yunji Chen\",\n    \"Olivier Temam\"\n  ],\n  \"context\": \"This article presents ShiDianNao, a CNN accelerator designed to be embedded next to CMOS or CCD sensors, eliminating DRAM accesses and being significantly more energy efficient than previous solutions. The design boasts a footprint of 4.86 mm\\u00b2, operating at 320 mW and delivers 30\\u00d7 faster performance than high-end GPUs.\",\n  \"other_metadata\": [\n    \"Keywords: CNN accelerator, energy efficiency, vision processing\",\n    \"Objective: Enhance energy efficiency by eliminating DRAM accesses with a closer sensor position.\",\n    \"Method: Hardware design with specific data access patterns leveraging CNN algorithm properties.\",\n    \"Results: 60\\u00d7 energy efficiency improvement and 30\\u00d7 performance boost over high-end GPUs.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"9":{"file_id":5858,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"c2b18efc-b49d-4e97-b35d-53d055bf6936_20250220011830.pdf","file_name":"document.pdf","file_hash":"1d20d3350f7699ffccea7acd5b01512d416bb8f98f9668ab096ab0309e0ec93d","file_ext":"pdf","upload_date":"2025-02-20T01:18:30.645453","pipeline_status":"retrying","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":null,"text":null,"score":null,"raw_url":null},"10":{"file_id":5865,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"84d15e2a-9452-467b-9fb8-8beadb598c42_20250220011842.pdf","file_name":"document.pdf","file_hash":"81a7d46b7112d2f9efbd424d41ea77374a5ff1907a5ccb897b44057c1912a961","file_ext":"pdf","upload_date":"2025-02-20T01:18:42.222413","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"DianNao Family: Energy-Efficient Hardware Accelerators for Machine Learning\",\n  \"reference\": \"Chen, Y., Chen, T., Xu, Z., Sun, N., & Temam, O. (2014). DianNao: A Small-Footprint, High-Throughput Accelerator for Ubiquitous Machine Learning. *Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)*, 49(4), 269\\u2013284.\",\n  \"doi\": null,\n  \"date\": \"March 2014\",\n  \"journal\": \"Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)\",\n  \"authors\": [\n    \"Yunji Chen\",\n    \"Tianshi Chen\",\n    \"Zhiwei Xu\",\n    \"Ninghui Sun\",\n    \"Olivier Temam\"\n  ],\n  \"context\": \"This article introduces the DianNao family of hardware accelerators designed for machine learning, especially neural networks, focusing on optimizing memory usage to enhance performance and energy efficiency.\",\n  \"other_metadata\": [\n    \"Keywords: Machine Learning, Hardware Accelerators, DianNao, Energy Efficiency, Neural Networks\",\n    \"Abstract available: Machine Learning tasks are becoming pervasive in a broad range of applications. The study explores efficient hardware design considerations to accelerate ML operations emphasizing memory management.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"11":{"file_id":5910,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"3f086a35-5e50-4690-915e-98d421fdfc61_20250220011852.pdf","file_name":"document.pdf","file_hash":"7064bccd65893297482e00c5ef261995f6d84f93bbe9fe919e0482a7c90e3527","file_ext":"pdf","upload_date":"2025-02-20T01:18:53.048700","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing\",\n  \"reference\": \"Albericio, J., Judd, P., Hetherington, T., Aamodt, T., Enright Jerger, N., & Moshovos, A. (2016). Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing. Proceedings of the 43rd Annual International Symposium on Computer Architecture, 1-12.\",\n  \"doi\": \"10.1145/2897937.2898064\",\n  \"date\": \"18-06-2016\",\n  \"journal\": null,\n  \"authors\": [\n    \"Jorge Albericio\",\n    \"Patrick Judd\",\n    \"Tayler Hetherington\",\n    \"Tor Aamodt\",\n    \"Natalie Enright Jerger\",\n    \"Andreas Moshovos\"\n  ],\n  \"context\": \"This paper presents Cnvlutin, a hardware accelerator designed to improve performance and energy efficiency of Deep Neural Networks by eliminating inutile computations involving zero-valued neurons, resulting in significant reductions in energy consumption and execution time.\",\n  \"other_metadata\": [\n    \"Keywords: Deep Neural Networks, Accelerator Design, Energy Efficiency, Convolutional Layers\",\n    \"Abstract: The Cnvlutin approach dynamically eliminates ineffectual multiplications in DNNs, resulting in a performance improvement of 1.24x to 1.55x over other accelerators while maintaining accuracy.\",\n    \"Experimental Results: Cnvlutin shows significant performance improvements, ranging from 24% to 55%, with substantial energy savings.\",\n    \"Motivation: A considerable amount of time and resources are wasted in computations involving zero neurons in DNNs. Cnvlutin addresses this issue effectively.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"12":{"file_id":5954,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"b1aabb1d-fd76-4291-918f-c3b50fbb7c3c_20250220011903.pdf","file_name":"document.pdf","file_hash":"a8bea102738045bb68faf24245a3222c883dc29f79be1395d0fe367f7f6f0aac","file_ext":"pdf","upload_date":"2025-02-20T01:19:04.178890","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"EIE: Efficient Inference Engine on Compressed Deep Neural Network\",\n  \"reference\": \"Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., Dally, W. J. (2023). EIE: Efficient Inference Engine on Compressed Deep Neural Network. IEEE/ACM.\",\n  \"doi\": null,\n  \"date\": \"2023-10-01\",\n  \"journal\": \"IEEE/ACM Transactions on Computer Systems\",\n  \"authors\": [\n    \"Song Han\",\n    \"Xingyu Liu\",\n    \"Huizi Mao\",\n    \"Jing Pu\",\n    \"Ardavan Pedram\",\n    \"Mark A. Horowitz\",\n    \"William J. Dally\"\n  ],\n  \"context\": \"The article introduces the EIE, an accelerator designed to perform inference using compressed deep neural networks efficiently. By leveraging techniques like pruning and weight sharing, EIE achieves significant energy and computational savings, demonstrating faster operation than traditional CPU and GPU implementations. Evaluation on multiple benchmarks proves EIE's superior efficiency.\",\n  \"other_metadata\": [\n    \"Keywords: Deep Learning, Model Compression, Hardware Acceleration, Algorithm-Hardware co-Design, ASIC\",\n    \"The paper presents significant advancements in energy efficiency and throughput by operating directly on compressed networks.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"13":{"file_id":6003,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"46c0fe25-b89e-4903-aa9c-c6d83a24f508_20250220011915.pdf","file_name":"document.pdf","file_hash":"0754bb738f7c02b10ee89da62d328f35653833a30f9def8aca75d2ae26ae178a","file_ext":"pdf","upload_date":"2025-02-20T01:19:15.341435","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks\",\n  \"reference\": \"Chen, Y.-H., Emer, J., & Sze, V. (Date TBD). Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks. Journal Name TBD.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Yu-Hsin Chen\",\n    \"Joel Emer\",\n    \"Vivienne Sze\"\n  ],\n  \"context\": \"The paper introduces a new dataflow, called row stationary (RS), for minimizing data movement energy consumption in CNNs on a spatial architecture. It evaluates different dataflows' energy efficiencies under the same hardware constraints and demonstrates the RS dataflow's efficiency on a fabricated chip.\",\n  \"other_metadata\": [\n    \"Keywords: Convolutional Neural Networks, dataflow, energy efficiency, spatial architecture, row stationary\",\n    \"Abstract: Addresses high energy consumption due to data movement in CNN processing and proposes a novel dataflow to mitigate the issue.\",\n    \"Experiments use AlexNet configurations to validate energy savings of the RS dataflow over existing ones.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"14":{"file_id":6045,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"a65cb288-d6ca-44ec-8824-e0a150d3cc7f_20250220011925.pdf","file_name":"document.pdf","file_hash":"12c3463ea95e1d1add365d45acf12c2203ee027e0e2fabda19958f93aabb71f1","file_ext":"pdf","upload_date":"2025-02-20T01:19:26.017326","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory\",\n  \"reference\": \"Kim, D., Kung, J., Chai, S., Yalamanchili, S., & Mukhopadhyay, S. (n.d.). Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory. Georgia Institute of Technology & SRI International.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Duckhwan Kim\",\n    \"Jaeha Kung\",\n    \"Sek Chai\",\n    \"Sudhakar Yalamanchili\",\n    \"Saibal Mukhopadhyay\"\n  ],\n  \"context\": \"This paper introduces the Neurocube, a digital neuromorphic architecture that integrates 3D high-density memory with a processor tier to improve computational efficiency and scalability in neural computing. The architecture leverages memory-centric neural computing (MCNC) to enhance data flow and power efficiency, bridging the programmability of GPGPUs with the efficiency of ASICs.\",\n  \"other_metadata\": [\n    \"Keywords: Neural nets, Neurocomputers, Neuromorphic computing\",\n    \"Neurocube achieves up to 132GOPs/s during inference within a 3.4W power envelope.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"15":{"file_id":6069,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"073592e9-3c31-48f1-8890-7885c0372543_20250220011937.pdf","file_name":"document.pdf","file_hash":"70372ca5c56c4c7066d8c34c0bd1434a50aee63b4439449de31b51022f058f87","file_ext":"pdf","upload_date":"2025-02-20T01:19:37.598610","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"ImageNet Classification with Deep Convolutional Neural Networks\",\n  \"reference\": \"Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, & K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems (Vol. 25, pp. 1097-1105). Curran Associates, Inc.\",\n  \"doi\": \"10.1145/3065386\",\n  \"date\": \"06-01-2012\",\n  \"journal\": \"Proceedings of the 25th International Conference on Neural Information Processing Systems\",\n  \"authors\": [\n    \"Alex Krizhevsky\",\n    \"Ilya Sutskever\",\n    \"Geoffrey E. Hinton\"\n  ],\n  \"context\": \"This paper discusses a large, deep convolutional neural network trained to classify images on ImageNet, outperforming previous records with a top-5 error rate of 17%. The network comprises five convolutional layers and three fully connected layers. Techniques to enhance model performance and minimize overfitting, including dropout and GPU acceleration, were employed.\",\n  \"other_metadata\": [\n    \"Keywords: Deep Learning, Convolutional Neural Networks, Image Classification, ImageNet, GPU acceleration\",\n    \"Abstract: The paper presents a breakthrough in image recognition performance with a deep convolutional neural network, significantly reducing error rates on the ImageNet challenge and prompting a paradigm shift in computer vision.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"16":{"file_id":6137,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"96950e88-0dbd-4409-9aeb-9f2dca45522b_20250220011949.pdf","file_name":"document.pdf","file_hash":"80124498bb9f5ffa6b4cc13dde435f6c3ff6486f0d601e5ade2b59e0ef65acb2","file_ext":"pdf","upload_date":"2025-02-20T01:19:49.219960","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks\",\n  \"reference\": \"Parashar, A., Rhu, M., Mukkara, A., Puglielli, A., Venkatesan, R., Khailany, B., Emer, J., Keckler, S. W., & Dally, W. J. (2017). SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks. In Proceedings of ISCA '17, Toronto, ON, Canada, June 24-28, 2017. https://doi.org/10.1145/3079856.3080254\",\n  \"doi\": \"10.1145/3079856.3080254\",\n  \"date\": \"24-06-2017\",\n  \"journal\": \"Proceedings of ISCA '17\",\n  \"authors\": [\n    \"Angshuman Parashar\",\n    \"Minsoo Rhu\",\n    \"Anurag Mukkara\",\n    \"Antonio Puglielli\",\n    \"Rangharajan Venkatesan\",\n    \"Brucek Khailany\",\n    \"Joel Emer\",\n    \"Stephen W. Keckler\",\n    \"William J. Dally\"\n  ],\n  \"context\": \"This paper introduces the SCNN accelerator architecture, aimed at improving the performance and energy efficiency of CNNs through a novel dataflow that exploits sparsity in weights and activations. SCNN achieves up to 2.7x performance improvement and 2.3x energy savings over dense CNN accelerators by eliminating unnecessary operations and effectively utilizing compressed sparse data.\",\n  \"other_metadata\": [\n    \"Keywords: Convolutional neural networks, accelerator architecture\",\n    \"ACM ISBN: 978-1-4503-4892-8/17/06\",\n    \"Published in Association for Computing Machinery\"\n  ]\n}","text":null,"score":null,"raw_url":null},"17":{"file_id":6160,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"8f869cd1-b2c5-4315-99ba-ae94d6abcbbd_20250220012000.pdf","file_name":"document.pdf","file_hash":"eea904dac2fc7d4e9b694f335d2b640c80990389f113d149a5475e09dc506eb0","file_ext":"pdf","upload_date":"2025-02-20T01:20:00.593685","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Tetris: Re-architecting Convolutional Neural Network Computation for Machine Learning Accelerators\",\n  \"reference\": \"Hang Lu, Xin Wei, Ning Lin, Guihai Yan, Xiaowei Li. (2018). Tetris: Re-architecting Convolutional Neural Network Computation for Machine Learning Accelerators. ICCAD '18, November 5\\u20138, 2018, San Diego, CA, USA.\",\n  \"doi\": \"10.1145/3240765.3240855\",\n  \"date\": \"05-11-2018\",\n  \"journal\": \"Proceedings of the International Conference on Computer-Aided Design (ICCAD)\",\n  \"authors\": [\n    \"Hang Lu\",\n    \"Xin Wei\",\n    \"Ning Lin\",\n    \"Guihai Yan\",\n    \"Xiaowei Li\"\n  ],\n  \"context\": \"This paper introduces 'Tetris', a deep learning accelerator designed to address inefficiencies in convolutional neural network computations by targeting ineffectual zero bits rather than zero values. The novel weight kneading technique and split-and-accumulate pattern improve inference speed and power efficiency compared to conventional methods.\",\n  \"other_metadata\": [\n    \"Keywords: Deep learning accelerators, convolutional neural networks, inference efficiency, zero bits, weight kneading, split-and-accumulate, machine learning, performance optimization, power efficiency\",\n    \"Conference: ICCAD '18, November 5\\u20138, 2018, San Diego, CA, USA\",\n    \"Abstract: Focuses on eliminating ineffectual computation caused by zero bits in CNNs through innovative architectural design, achieving up to 1.50x speed increase and 5.33x power efficiency over state-of-the-art baselines.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"18":{"file_id":6232,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"79ca761b-b063-4069-bd9b-b2f6656403cb_20250220012011.pdf","file_name":"document.pdf","file_hash":"ad113c3701d4f93c8bd7556ba22bee12912e82b7f70ac1919181d5259ff72c4c","file_ext":"pdf","upload_date":"2025-02-20T01:20:11.446266","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Deep Neural Network Approximation for Custom Hardware: Where We've Been, Where We're Going\",\n  \"reference\": \"Erwei Wang, James J. Davis, Ruizhe Zhao, Ho-Cheung Ng, Xinyu Niu, Wayne Luk, Peter Y. K. Cheung, and George A. Constantinides. (2019). Deep Neural Network Approximation for Custom Hardware: Where We've Been, Where We're Going. ACM Comput. Surv. 52, 2, Article 40 (May 2019), 39 pages. doi:10.1145/3309551\",\n  \"doi\": \"10.1145/3309551\",\n  \"date\": \"May-2019\",\n  \"journal\": \"ACM Computing Surveys\",\n  \"authors\": [\n    \"Erwei Wang\",\n    \"James J. Davis\",\n    \"Ruizhe Zhao\",\n    \"Ho-Cheung Ng\",\n    \"Xinyu Niu\",\n    \"Wayne Luk\",\n    \"Peter Y. K. Cheung\",\n    \"George A. Constantinides\"\n  ],\n  \"context\": \"The article evaluates approximation methods for transforming deep neural networks (DNNs) into hardware-efficient alternatives, focusing on their deployment in custom hardware such as FPGAs and ASICs. It compares and contrasts approximation strategies for convolutional and recurrent networks and highlights potential research directions based on current trends.\",\n  \"other_metadata\": [\n    \"Keywords: FPGAs, ASICs, approximation methods, convolutional neural networks, recurrent neural networks\",\n    \"ACM Classification: Surveys and overviews; Neural networks; Hardware accelerators\",\n    \"Support: UK EPSRC Grants, European Union Horizon 2020 Grant, various scholarships and technological collaborations\"\n  ]\n}","text":null,"score":null,"raw_url":null},"19":{"file_id":6255,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"c7f46b29-5d07-4d0e-a2b1-6efbde996a96_20250220012027.pdf","file_name":"document.pdf","file_hash":"d1d86fa7f94030710e19ae0ad6b3b4228e693ad77683f8e69f7e8b226e084b6e","file_ext":"pdf","upload_date":"2025-02-20T01:20:28.172719","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture\",\n  \"reference\": \"Yakun Sophia Shao, Jason Clemons, Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J. Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. 2019. Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture . In The 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-52), October 12\\u201316, 2019, Columbus, OH, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3352460.3358302\",\n  \"doi\": \"10.1145/3352460.3358302\",\n  \"date\": \"12-10-2019\",\n  \"journal\": \"The 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-52)\",\n  \"authors\": [\n    \"Yakun Sophia Shao\",\n    \"Jason Clemons\",\n    \"Rangharajan Venkatesan\",\n    \"Brian Zimmer\",\n    \"Matthew Fojtik\",\n    \"Nan Jiang\",\n    \"Ben Keller\",\n    \"Alicia Klinefelter\",\n    \"Nathaniel Pinckney\",\n    \"Priyanka Raina\",\n    \"Stephen G. Tell\",\n    \"Yanqing Zhang\",\n    \"William J. Dally\",\n    \"Joel Emer\",\n    \"C. Thomas Gray\",\n    \"Brucek Khailany\",\n    \"Stephen W. Keckler\"\n  ],\n  \"context\": \"The article presents Simba, a scalable deep-learning inference accelerator utilizing multi-chip-module (MCM) integration. It addresses the challenges of using fine-grained chiplets for deep learning, emphasizing a non-uniform tiling strategy to enhance data locality and mitigate inter-chiplet communication overheads, achieving significant computational efficiency in large-scale systems.\",\n  \"other_metadata\": [\n    \"Keywords: Multi-chip module, neural networks, accelerator architecture\",\n    \"CCS Concepts: Interconnection architectures, Multicore architectures, Neural networks, Data flow architectures, Special purpose systems\",\n    \"ACM ISBN: 978-1-4503-6938-1/19/10\"\n  ]\n}","text":null,"score":null,"raw_url":null},"20":{"file_id":6343,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"af0d8596-f143-4b1c-ac47-737c0d28018a_20250220012038.pdf","file_name":"document.pdf","file_hash":"92da4dbb42bb48ccb96cf47aa058b68dd532ad239a65bec9405ae7c6551ac4c8","file_ext":"pdf","upload_date":"2025-02-20T01:20:38.963191","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"A Method to Estimate the Energy Consumption of Deep Neural Networks\",\n  \"reference\": \"Yang, T.-J., Chen, Y.-H., Emer, J., & Sze, V. A Method to Estimate the Energy Consumption of Deep Neural Networks. Massachusetts Institute of Technology, Cambridge, MA, USA.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Tien-Ju Yang\",\n    \"Yu-Hsin Chen\",\n    \"Joel Emer\",\n    \"Vivienne Sze\"\n  ],\n  \"context\": \"The article proposes an energy estimation methodology for Deep Neural Networks, focusing on architecture, sparsity, and bitwidth to guide the design of energy-efficient DNNs. The methodology evaluates energy consumption in relation to data movement in hardware memory hierarchies and provides insights into efficient network design.\",\n  \"other_metadata\": [\n    \"Keywords: Deep learning, energy estimation, machine learning, energy-efficient techniques, memory optimization.\",\n    \"Abstract: Discusses challenges in estimating energy consumption for DNNs, focusing on data movement rather than just computation, providing a methodology for energy estimation at the network and layer level.\",\n    \"Index Terms: Deep learning, deep neural network, energy estimation, energy metric, machine learning.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"21":{"file_id":6358,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"88d696ee-ab83-4e17-a896-6a9e79e928ec_20250220012049.pdf","file_name":"document.pdf","file_hash":"df44ee09a8910f0f705a42b2567b348b59d2990c233cee8f9da01388bcf05f1a","file_ext":"pdf","upload_date":"2025-02-20T01:20:49.409252","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"A Low Bit-width Parameter Representation Method for Hardware-oriented Convolution Neural Networks\",\n  \"reference\": \"Chen, Q., Xin, C., Zou, C., Wang, X., & Wang, B. (2023). A Low Bit-width Parameter Representation Method for Hardware-oriented Convolutional Neural Networks. Unpublished manuscript. School of ECE, Peking University Shenzhen Graduate School.\",\n  \"doi\": null,\n  \"date\": \"2023\",\n  \"journal\": null,\n  \"authors\": [\n    \"Qiang Chen\",\n    \"Chen Xin\",\n    \"Chenglong Zou\",\n    \"Xinan Wang\",\n    \"Bo Wang\"\n  ],\n  \"context\": \"The article proposes a method to represent convolutional neural networks (CNNs) with a reduced bit-width to make them more suitable for resource-limited devices. This new approach allows for parameter compression without re-training, with a accuracy loss less than 1%.\",\n  \"other_metadata\": [\n    \"Keywords: CNNs, hardware, low bit-width, parameter compression, FPGA\",\n    \"Abstract: Discusses the reduction of parameters and energy consumption in CNNs using low bit-width representations for hardware efficiency.\",\n    \"Publication Institution: School of ECE, Peking University Shenzhen Graduate School\",\n    \"Acknowledgments: Supported by R&D Project of Shenzhen Government with project IDs JCYJ20160229094148396 and JCYJ20160428153956266.\",\n    \"Reference List: Includes works on hardware-oriented CNN approximation and deep compression techniques.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"22":{"file_id":6424,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"062209ac-baa6-45da-b320-ffd67d9377e7_20250220012059.pdf","file_name":"document.pdf","file_hash":"6e7b2baa56d5e0f0ed7459017b33101d36558b614ecb7bb24476b92a6b484985","file_ext":"pdf","upload_date":"2025-02-20T01:20:59.602270","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\",\n  \"reference\": \"Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., & Kalenichenko, D. (Year). Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. Conference/Journal Name, Volume(Issue), Page Range.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Benoit Jacob\",\n    \"Skirmantas Kligys\",\n    \"Bo Chen\",\n    \"Menglong Zhu\",\n    \"Matthew Tang\",\n    \"Andrew Howard\",\n    \"Hartwig Adam\",\n    \"Dmitry Kalenichenko\"\n  ],\n  \"context\": \"The paper presents a quantization scheme enabling neural network inference using integer-only arithmetic, thus improving efficiency on common hardware. This scheme was co-designed with a training procedure to retain model accuracy after quantization. Significant performance improvements are demonstrated on MobileNets for ImageNet classification and COCO detection using popular CPUs.\",\n  \"other_metadata\": [\n    \"Keywords: Quantization, Neural Networks, Integer Arithmetic, MobileNet, ImageNet, COCO Detection\",\n    \"Objective: To enhance on-device inference efficiency by using integer-only arithmetic.\",\n    \"Methods: Co-design quantization and training to preserve model accuracy.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"23":{"file_id":6439,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"96482662-7be2-490e-984e-30970b6710d0_20250220012110.pdf","file_name":"document.pdf","file_hash":"84daf4e98faaed63518c3cb1ded0cc1261f211b84d90a648a1548f4af369dac9","file_ext":"pdf","upload_date":"2025-02-20T01:21:11.022399","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"PuDianNao: A Polyvalent Machine Learning Accelerator\",\n  \"reference\": \"Daofu Liu, Jinhong Zhou, Xiaobing Feng, Tianshi Chen, Shengyuan Zhou, Xuehai Zhou, Shaoli Liu, Olivier Teman, Yunji Chen. (2015). PuDianNao: A Polyvalent Machine Learning Accelerator. ASPLOS '15, March 14\\u201318, 2015, Istanbul, Turkey. ACM.\",\n  \"doi\": \"10.1145/2694344.2694358\",\n  \"date\": \"14-03-2015\",\n  \"journal\": \"ASPLOS '15\",\n  \"authors\": [\n    \"Daofu Liu\",\n    \"Jinhong Zhou\",\n    \"Xiaobing Feng\",\n    \"Tianshi Chen\",\n    \"Shengyuan Zhou\",\n    \"Xuehai Zhou\",\n    \"Shaoli Liu\",\n    \"Olivier Teman\",\n    \"Yunji Chen\"\n  ],\n  \"context\": \"The article presents PuDianNao, an ML accelerator supporting multiple techniques like k-means, k-nearest neighbors, naive bayes, and more. It highlights the adaptability of PuDianNao to various ML techniques, demonstrating its superior speed and energy efficiency compared to GPUs.\",\n  \"other_metadata\": [\n    \"Keywords: Machine Learning, Hardware Accelerator, Energy Efficiency, PuDianNao\",\n    \"Abstract: This study introduces an efficient ML accelerator addressing energy efficiency limitations of CPUs and GPUs by accommodating multiple ML techniques.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"24":{"file_id":6466,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"8f0f2741-1de4-4d31-81eb-731730fe63dc_20250220012121.pdf","file_name":"document.pdf","file_hash":"86bb4dcdc3403ce201aff1d6305ce6020516351faa736747a356f0b5f9b44579","file_ext":"pdf","upload_date":"2025-02-20T01:21:21.540304","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Understanding the Impact of Precision Quantization on the Accuracy and Energy of Neural Networks\",\n  \"reference\": \"Hashemi, S., Anthony, N., Tann, H., Bahar, R. I., & Reda, S. (n.d.). Understanding the Impact of Precision Quantization on the Accuracy and Energy of Neural Networks. Brown University. Retrieved from Brown University Engineering Journal.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": \"Brown University Engineering Journal\",\n  \"authors\": [\n    \"Soheil Hashemi\",\n    \"Nicholas Anthony\",\n    \"Hokchhay Tann\",\n    \"R. Iris Bahar\",\n    \"Sherief Reda\"\n  ],\n  \"context\": \"The article investigates the effects of different bit precisions on neural networks' accuracy and energy efficiency. It evaluates various numerical precisions ranging from floating-point to binary representation, focusing on energy consumption, memory footprint, and accuracy trade-offs. The study aims to highlight how precision scaling can achieve efficient designs with minimal accuracy loss.\",\n  \"other_metadata\": [\n    \"Keywords: neural networks, precision quantization, energy efficiency, accuracy, hardware accelerators\",\n    \"Abstract: A comprehensive study evaluating different precision levels in neural networks, focusing on the balance between energy efficiency and accuracy.\",\n    \"Methodology: Evaluated a range of precisions, training techniques, and hardware configurations on popular datasets including MNIST, SVHN, CIFAR-10.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"25":{"file_id":6467,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"4210e6b4-321d-44da-9797-a27ed5e2cb87_20250220012132.pdf","file_name":"document.pdf","file_hash":"86e0a78e30235c442b409508f5c39d5a487d6ddabb6a9041715ca987ecf382ce","file_ext":"pdf","upload_date":"2025-02-20T01:21:32.384340","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Exploration of Low Numeric Precision Deep Learning Inference Using Intel\\u00ae FPGAs\",\n  \"reference\": \"Colangelo, P., Mishra, A., Nasiri, N., Margala, M., Nurvitadhi, E., & Nealis, K. (n.d.). Exploration of Low Numeric Precision Deep Learning Inference Using Intel\\u00ae FPGAs.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Philip Colangelo\",\n    \"Asit Mishra\",\n    \"Nasibeh Nasiri\",\n    \"Martin Margala\",\n    \"Eriko Nurvitadhi\",\n    \"Kevin Nealis\"\n  ],\n  \"context\": \"The article discusses a framework using Intel FPGAs to enhance deep learning inference through low numeric precision networks, focusing on trade-offs between throughput and accuracy.\",\n  \"other_metadata\": [\n    \"Keywords: FPGA, Arria 10, Stratix 10, Deep Learning, Low Precision Neural Network, CNN\",\n    \"Methods: Utilization of ternary and binary weights in neural networks using FPGA-based frameworks\",\n    \"Results: Demonstrated the effectiveness of low numeric precision networks on FPGA with case studies on AlexNet and ResNet-34\"\n  ]\n}","text":null,"score":null,"raw_url":null},"26":{"file_id":6518,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"5698d43a-a7f9-44a7-aa19-c107c8dfddad_20250220012143.pdf","file_name":"document.pdf","file_hash":"703b7f7e8ae6b8ad21193a5f49462ef53618bb79cf9002da53dfb06ea810c3c4","file_ext":"pdf","upload_date":"2025-02-20T01:21:43.242218","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Deep learning in robotics: a review of recent research\",\n  \"reference\": \"Pierson, H. A., & Gashley, M. S. (2017). Deep learning in robotics: a review of recent research. [Pending further citation details]\",\n  \"doi\": null,\n  \"date\": \"19-07-2017\",\n  \"journal\": null,\n  \"authors\": [\n    \"Harry A. Pierson\",\n    \"Michael S. Gashley\"\n  ],\n  \"context\": \"The article reviews the integration of deep learning in robotics, discussing the applications, benefits, and limitations of adopting deep learning models in physical robotic systems. It highlights contemporary research, emphasizing the advancements and challenges encountered, to inspire further exploration in this evolving domain.\",\n  \"other_metadata\": [\n    \"Keywords: Deep neural networks; artificial intelligence; human\\u2013robot interaction\",\n    \"Received: 24 December 2016\",\n    \"Revised: 26 May 2017\",\n    \"Department of Industrial Engineering, University of Arkansas, Fayetteville, AR, USA\",\n    \"Department of Computer Science and Computer Engineering, University of Arkansas, Fayetteville, AR, USA\",\n    \"Acknowledges at least 30 papers published between 2014 and the time of writing on deep learning applications in robotics\"\n  ]\n}","text":null,"score":null,"raw_url":null},"27":{"file_id":6544,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"11c9ee84-d1e7-4173-a805-07bd1ebc7c62_20250220012154.pdf","file_name":"document.pdf","file_hash":"e220786ea640624d3775dd8a8783edd7cf267c91567ce3ea447464f3d47936ba","file_ext":"pdf","upload_date":"2025-02-20T01:21:54.228614","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Memristive Boltzmann Machine: A Hardware Accelerator for Combinatorial Optimization and Deep Learning\",\n  \"reference\": \"Bojnordi, M. N., & Ipek, E. (Year). Memristive Boltzmann Machine: A Hardware Accelerator for Combinatorial Optimization and Deep Learning. [Publisher details if available].\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Mahdi Nazm Bojnordi\",\n    \"Engin Ipek\"\n  ],\n  \"context\": \"The article discusses the development of a hardware accelerator for Boltzmann Machines using memristive technology, specifically RRAM, to enhance the performance of combinatorial optimization and deep learning tasks. It proposes utilizing the memory characteristics of RRAM for parallel computation directly within memory arrays, bypassing data exchange with computational units. The solution promises significant performance and energy improvements over conventional systems.\",\n  \"other_metadata\": [\n    \"Keywords: Memristive Technology, Boltzmann Machine, Hardware Accelerator, Combinatorial Optimization, Deep Learning, RRAM\",\n    \"Abstract: The core focus of this paper is on leveraging memristive devices to implement a hardware accelerator for Boltzmann Machines. The proposal promises improved performance and energy efficiency in solving classical optimization problems like graph partitioning and deep belief network applications.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"28":{"file_id":6584,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"96fe7b08-e8c0-4888-851a-28afb7dbd150_20250220012205.pdf","file_name":"document.pdf","file_hash":"99220b2b38f7f7faf2e3e108a6d2286b715d166c5e034cb3c6c1dea9a74774bc","file_ext":"pdf","upload_date":"2025-02-20T01:22:05.666860","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Survey and Benchmarking of Machine Learning Accelerators\",\n  \"reference\": \"Reuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., & Kepner, J. (2019). Survey and Benchmarking of Machine Learning Accelerators. MIT Lincoln Laboratory Supercomputing Center.\",\n  \"doi\": null,\n  \"date\": \"N/A\",\n  \"journal\": \"N/A\",\n  \"authors\": [\n    \"Albert Reuther\",\n    \"Peter Michaleas\",\n    \"Michael Jones\",\n    \"Vijay Gadepally\",\n    \"Siddharth Samsi\",\n    \"Jeremy Kepner\"\n  ],\n  \"context\": \"The article surveys the current state of machine learning processors and accelerators, analyzing their performance and power consumption. It compares these processors while considering factors such as numerical precision, training vs. inference, and SWaP constraints for embedded applications.\",\n  \"other_metadata\": [\n    \"Keywords: Machine learning, GPU, TPU, dataflow, accelerator, embedded inference\",\n    \"Affiliation: MIT Lincoln Laboratory Supercomputing Center, Lexington, MA, USA\",\n    \"Abstract: Discusses the advances in multicore processors and accelerators that have expanded machine learning applications.\",\n    \"Support: Assistant Secretary of Defense for Research and Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001\",\n    \"Figure 1: Canonical AI architecture diagram.\",\n    \"Figure 2: Scatter plot of processor capabilities.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"29":{"file_id":6626,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"515a8b27-07f3-4805-82f9-12412722960c_20250220012216.pdf","file_name":"document.pdf","file_hash":"15e6cb8c5ce3f26ea83c1bc3ec3cb57adfeb749b1639c37f769e9b2cd4819a86","file_ext":"pdf","upload_date":"2025-02-20T01:22:17.105270","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving\",\n  \"reference\": \"Chenyi Chen, Ari Seff, Alain Kornhauser, & Jianxiong Xiao. (Date unknown). DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving. Retrieved from http://deepdriving.cs.princeton.edu\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Chenyi Chen\",\n    \"Ari Seff\",\n    \"Alain Kornhauser\",\n    \"Jianxiong Xiao\"\n  ],\n  \"context\": \"This paper presents a new 'direct perception' approach for autonomous driving, aiming to estimate the affordance of road/traffic states rather than scene parsing or direct mapping to actions. Using deep CNNs trained on both virtual and real datasets, the model demonstrates effectiveness in a variety of driving environments.\",\n  \"other_metadata\": [\n    \"Keywords: Autonomous driving, Direct perception, Affordance indicators, Deep learning, CNN, TORCS\",\n    \"The study uses 12 hours of driving data from a video game and real-world datasets like KITTI.\",\n    \"It positions itself as a compromise between high-level scene understanding and behavior reflex approaches.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"30":{"file_id":6659,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"a80092ab-79e1-4993-b6ec-305bce97d18c_20250220012228.pdf","file_name":"document.pdf","file_hash":"8e5c4a8cb23645552eba065b573a58cfb8308617afd5ad65897a5ffff9f91e42","file_ext":"pdf","upload_date":"2025-02-20T01:22:28.959651","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"A Survey of Deep Learning Methods for Cyber Security\",\n  \"reference\": \"Berman, D. S., Buczak, A. L., Chavis, J. S., & Corbett, C. L. (2019). A survey of deep learning methods for cyber security. *Information, 10*(4), 122. doi: 10.3390/info10040122\",\n  \"doi\": \"10.3390/info10040122\",\n  \"date\": \"02-04-2019\",\n  \"journal\": \"Information\",\n  \"authors\": [\n    \"Daniel S. Berman\",\n    \"Anna L. Buczak\",\n    \"Jeffrey S. Chavis\",\n    \"Cherita L. Corbett\"\n  ],\n  \"context\": \"This survey paper reviews deep learning methods applied to cyber security, detailing several deep learning techniques and how they improve cyber defense systems. It discusses a variety of DL methods like autoencoders, DBNs, and GANs, and their application to threats like malware and intrusions.\",\n  \"other_metadata\": [\n    \"Keywords: cyber analytics, deep learning, deep neural networks, deep autoencoders, deep belief networks, restricted Boltzmann machines, convolutional neural networks\",\n    \"Affiliation: Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA\",\n    \"The paper covers a broad array of attack types including malware, spam, insider threats, network intrusions, false data injection, and malicious domains.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"31":{"file_id":6687,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"f8a19085-a79a-49d9-b7ef-29fe9ea2242f_20250220012239.pdf","file_name":"document.pdf","file_hash":"d8e1df224a7d1e3cb68d403d466999ee4359b3b998bf60ae930f3062dc414f38","file_ext":"pdf","upload_date":"2025-02-20T01:22:39.878597","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars\",\n  \"reference\": \"Shafiee, A., Nag, A., Muralimanohar, N., Balasubramonian, R., Strachan, J. P., Hu, M., Williams, R. S., & Srikumar, V. (2023). ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars. Journal of Computer Architecture.\",\n  \"doi\": null,\n  \"date\": \"04-10-2023\",\n  \"journal\": \"Journal of Computer Architecture\",\n  \"authors\": [\n    \"Ali Shafiee\",\n    \"Anirban Nag\",\n    \"Naveen Muralimanohar\",\n    \"Rajeev Balasubramonian\",\n    \"John Paul Strachan\",\n    \"Miao Hu\",\n    \"R. Stanley Williams\",\n    \"Vivek Srikumar\"\n  ],\n  \"context\": \"The ISAAC architecture leverages memristor crossbar arrays to implement an optimized CNN accelerator performing dot-product operations with improved efficiency over existing architectures like DaDianNao by utilizing in-situ analog computation.\",\n  \"other_metadata\": [\n    \"Keywords: CNN, DNN, memristor, analog, neural accelerator\",\n    \"Abstract: A CNN accelerator using memristor crossbars for analog arithmetic, achieving notable performance and efficiency improvements over DaDianNao.\",\n    \"Page Range: 1-12\"\n  ]\n}","text":null,"score":null,"raw_url":null},"32":{"file_id":6700,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"b4b25d67-c7ce-413a-9136-2db2d40e8b14_20250220012251.pdf","file_name":"document.pdf","file_hash":"1dc6978c1e2e4931aa1bc4cba15d7ef2fad490dddb18353490151dc927105b46","file_ext":"pdf","upload_date":"2025-02-20T01:22:51.453412","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"PRIME: A Novel Processing-in-memory Architecture for Neural Network Computation in ReRAM-based Main Memory\",\n  \"reference\": \"Chi, P., Li, S., Xu, C., Zhang, T., Zhao, J., Liu, Y., Wang, Y., & Xie, Y. (n.d.). PRIME: A Novel Processing-in-memory Architecture for Neural Network Computation in ReRAM-based Main Memory.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Ping Chi\",\n    \"Shuangchen Li\",\n    \"Cong Xu\",\n    \"Tao Zhang\",\n    \"Jishen Zhao\",\n    \"Yongpan Liu\",\n    \"Yu Wang\",\n    \"Yuan Xie\"\n  ],\n  \"context\": \"The article discusses PRIME, a processing-in-memory architecture leveraging ReRAM to efficiently accelerate neural network applications. Using ReRAM crossbar arrays as computation units, PRIME offers significant performance and energy gains compared to traditional architectures.\",\n  \"other_metadata\": [\n    \"Keywords: processing in memory, neural network, resistive random access memory\",\n    \"Abstract: Processing-in-memory (PIM) is a promising solution to address the 'memory wall' challenges for future computer systems. The PRIME architecture shows significant improvements in performance and energy saving across machine learning benchmarks.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"33":{"file_id":6789,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"c21f5716-ce1f-4335-959b-2b580e1f2cc7_20250220012301.pdf","file_name":"document.pdf","file_hash":"fd9589f3bee5f81cd2369dd3b39e741168d1dce91b1312be5acc04dc2e292c43","file_ext":"pdf","upload_date":"2025-02-20T01:23:01.971828","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Timeloop: A Systematic Approach to DNN Accelerator Evaluation\",\n  \"reference\": \"Parashar, A., Raina, P., Shao, Y. S., Chen, Y. H., Ying, V. A., Mukkara, A., Venkatesan, R., Khailany, B., Keckler, S. W., & Emer, J. (Year). Timeloop: A Systematic Approach to DNN Accelerator Evaluation.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Angshuman Parashar\",\n    \"Priyanka Raina\",\n    \"Yakun Sophia Shao\",\n    \"Yu-Hsin Chen\",\n    \"Victor A. Ying\",\n    \"Anurag Mukkara\",\n    \"Rangharajan Venkatesan\",\n    \"Brucek Khailany\",\n    \"Stephen W. Keckler\",\n    \"Joel Emer\"\n  ],\n  \"context\": \"The article introduces Timeloop, a tool for evaluating DNN accelerators by simulating a wide range of hardware topologies to project performance and energy efficiency. By optimizing data scheduling and staging, Timeloop allows for comparisons between architectures, highlighting the importance of dataflow and memory hierarchy co-design.\",\n  \"other_metadata\": [\n    \"Keywords: DNN accelerators, architecture design, energy efficiency, data scheduling, hardware simulation\",\n    \"Abstract: This paper describes Timeloop's underlying models and algorithms, and case studies providing insights into DNN architecture design.\",\n    \"Insights: The importance of dataflow and memory hierarchy co-design in optimizing energy efficiency.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"34":{"file_id":6856,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"52e12a77-f4f2-4f0f-bd29-e94f917242cc_20250220012313.pdf","file_name":"document.pdf","file_hash":"4c190ace06ac6ee6ba5a4bca465712257757fd6d188671d7b27dce9bc4035937","file_ext":"pdf","upload_date":"2025-02-20T01:23:13.387257","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Computing's Energy Problem (and what we can do about it)\",\n  \"reference\": \"Horowitz, M. (2023). Computing's Energy Problem (and what we can do about it). Departments of Electrical Engineering and Computer Science, Stanford University.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Mark Horowitz\"\n  ],\n  \"context\": \"This article discusses the evolution and challenges of scaling computing in terms of power constraints. It explores the growing energy limitations due to CMOS technology scaling and explains why parallel processing and specialized applications are essential to address these challenges. The paper highlights that improving energy efficiency will require more than parallelism and emphasizes the need for better applications and hardware harmony.\",\n  \"other_metadata\": [\n    \"Keywords: computing energy, CMOS technology, processor scaling, parallel processing, energy efficiency\",\n    \"Abstract: Examines the history of computing performance and energy consumption and discusses approaches to address energy efficiency in the context of limited technology scaling.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"35":{"file_id":6897,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"e1af5084-4972-4f6e-9038-43ab10e31e2f_20250220012325.pdf","file_name":"document.pdf","file_hash":"2fa3eeafde4fd5cbb1140c6a482e5aa0540e3fe35281799c22db940436009c0b","file_ext":"pdf","upload_date":"2025-02-20T01:23:25.665460","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"14.2 DNPU: An 8.1TOPS/W Reconfigurable CNN-RNN Processor for General-Purpose Deep Neural Networks\",\n  \"reference\": \"Shin, D., Lee, J., Lee, J., & Yoo, H.-J. (2017). 14.2 DNPU: An 8.1TOPS/W Reconfigurable CNN-RNN Processor for General-Purpose Deep Neural Networks. Presented at ISSCC 2017.\",\n  \"doi\": null,\n  \"date\": \"07-02-2017\",\n  \"journal\": \"ISSCC 2017\",\n  \"authors\": [\n    \"Dongjoo Shin\",\n    \"Jinmook Lee\",\n    \"Jinsu Lee\",\n    \"Hoi-Jun Yoo\"\n  ],\n  \"context\": \"The paper introduces a reconfigurable deep neural processor unit (DNPU) that supports both CNNs and RNNs with high energy efficiency. It explores architectural solutions to common performance and efficiency problems in DNN processing by proposing a dynamic, adaptive design that facilitates optimal computation with various neural network layers.\",\n  \"other_metadata\": [\n    \"Keywords: deep neural networks, reconfigurable architecture, CNN-RNN processor, energy efficiency\",\n    \"Abstract: Detailed exploration of 8.1TOPS/W DNPU architecture, components, and performance metrics.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"36":{"file_id":6911,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"7c90cc1d-9f4e-4ef5-9110-facec8a05f58_20250220012339.pdf","file_name":"document.pdf","file_hash":"f40a617a4b4fda11785a714d7227f55ac0594c86cdbfb500c7dbb631dad15c2a","file_ext":"pdf","upload_date":"2025-02-20T01:23:39.429868","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"14.5 ENVISION: A 0.26-to-10TOPS/W Subword-Parallel Dynamic-Voltage-Accuracy-Frequency-Scalable Convolutional Neural Network Processor in 28nm FDSOI\",\n  \"reference\": \"Moons, B., Uytterhoeven, R., Dehaene, W., & Verhelst, M. (2017). An energy-scalable ConvNet processor. ISSCC 2017 Paper.\",\n  \"doi\": null,\n  \"date\": \"2017\",\n  \"journal\": \"ISSCC 2017\",\n  \"authors\": [\n    \"Bert Moons\",\n    \"Roel Uytterhoeven\",\n    \"Wim Dehaene\",\n    \"Marian Verhelst\"\n  ],\n  \"context\": \"The article presents ENVISION, a ConvNet processor with dynamic energy scaling capabilities geared towards always-on visual recognition tasks in wearable devices. It achieves scalability of up to 10TOPS/W through hierarchical processing techniques and DVAFS, thereby enhancing energy efficiency.\",\n  \"other_metadata\": [\n    \"Keywords: ConvNet processor, Energy-efficiency, Dynamic Voltage Scaling, FDSOI.\",\n    \"Abstract: Envision enables wearable devices to perform visual recognition tasks efficiently by dynamically adjusting energy usage according to processing needs.\",\n    \"Figures: The article includes figures illustrating architectural innovations and comparing energy efficiency with other models.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"37":{"file_id":6919,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"60a496ca-bd46-4dcc-832f-649f0455792b_20250220012351.pdf","file_name":"document.pdf","file_hash":"d706a832ff0468ff252722febfa9229ea9b0154ae3019d3d8dd7225271c1cfd0","file_ext":"pdf","upload_date":"2025-02-20T01:23:51.928583","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"UNPU: A 50.6TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision\",\n  \"reference\": \"Lee, J., Kim, C., Kang, S., Shin, D., Kim, S., & Yoo, H.-J. (2018). UNPU: A 50.6TOPS/W Unified Deep Neural Network Accelerator with 1b-to-16b Fully-Variable Weight Bit-Precision. In Proceedings of the 2018 IEEE International Solid-State Circuits Conference (ISSCC), pp. 218-219, 2018.\",\n  \"doi\": null,\n  \"date\": \"2018\",\n  \"journal\": \"Proceedings of the 2018 IEEE International Solid-State Circuits Conference (ISSCC)\",\n  \"authors\": [\n    \"Jinmook Lee\",\n    \"Changhyeon Kim\",\n    \"Sanghoon Kang\",\n    \"Dongjoo Shin\",\n    \"Sangyeob Kim\",\n    \"Hoi-Jun Yoo\"\n  ],\n  \"context\": \"The paper introduces UNPU, a unified neural processing unit that provides support for CLs, RLs, and FCLs with a fully variable weight bit-precision from 1b to 16b. It demonstrates an enhanced energy-efficient architecture that significantly improves performance across various neural network operations compared to previous models.\",\n  \"other_metadata\": [\n    \"Keywords: DNN Accelerator, weight bit-precision, neural processing unit, energy efficiency\",\n    \"Abstract: The authors present a DNN accelerator capable of handling a range of weight bit-precision settings, optimizing performance and energy efficiency for mobile deep learning applications.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"38":{"file_id":6927,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"ff6b5d1d-c427-43e5-a0c8-37fe7fbc3658_20250220012402.pdf","file_name":"document.pdf","file_hash":"0217e2cb7585f4b0cd48bec0c04c143a53b2a29739435ab19e6aa39874090122","file_ext":"pdf","upload_date":"2025-02-20T01:24:02.281432","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"7.7 LNPU: A 25.3TFLOPS/W Sparse Deep-Neural-Network Learning Processor with Fine-Grained Mixed Precision of FP8-FP16\",\n  \"reference\": \"Jinsu Lee, Juhyoung Lee, Donghyeon Han, Jinmook Lee, Gwangtae Park, & Hoi-Jun Yoo.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Jinsu Lee\",\n    \"Juhyoung Lee\",\n    \"Donghyeon Han\",\n    \"Jinmook Lee\",\n    \"Gwangtae Park\",\n    \"Hoi-Jun Yoo\"\n  ],\n  \"context\": \"This article details a novel deep-neural-network learning processor (LNPU) that achieves 25.3TFLOPS/W efficiency through the use of fine-grained mixed precision (FP8-FP16). The LNPU is designed for energy-efficient local learning on edge and mobile devices, leveraging sparsity in inputs to enhance performance.\",\n  \"other_metadata\": [\n    \"Keywords: Deep Neural Network, Processor, Energy Efficiency, Mixed Precision, Sparsity, FP8, FP16\",\n    \"Abstract: Describes the LNPU's ability to perform local DNN learning on edge devices by utilizing sparsity in CNN inputs such as VGG16, AlexNet, and ResNet-18. Highlights its energy efficiency by employing a fine-grained mixed precision approach.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"39":{"file_id":6937,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"06351ded-b0cd-46fb-a659-af27084bee05_20250220012412.pdf","file_name":"document.pdf","file_hash":"711cb71ecb23687c4b5f01b3ef02875bbb177de87083df1656b8a6ba17a51124","file_ext":"pdf","upload_date":"2025-02-20T01:24:12.881406","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"A 65nm 1.1-to-9.1TOPS/W Hybrid-Digital-Mixed-Signal Computing Platform for Accelerating Model-Based and Model-Free Swarm Robotics\",\n  \"reference\": \"Cao, N., Chang, M., & Raychowdhury, A. (2019). A 65nm 1.1-to-9.1TOPS/W Hybrid-Digital-Mixed-Signal Computing Platform for Accelerating Model-Based and Model-Free Swarm Robotics. In ISSCC 2019.\",\n  \"doi\": null,\n  \"date\": \"19-02-2019\",\n  \"journal\": \"International Solid-State Circuits Conference (ISSCC)\",\n  \"authors\": [\n    \"Ningyuan Cao\",\n    \"Muya Chang\",\n    \"Arijit Raychowdhury\"\n  ],\n  \"context\": \"The paper presents a scalable and energy-efficient platform capable of supporting real-time swarm intelligence. The hybrid-digital-mixed-signal (HDMS) architecture addresses both model-based and model-free swarm algorithms, demonstrating high energy efficiency and scalability across various swarm sizes.\",\n  \"other_metadata\": [\n    \"Keywords: swarm robotics, hybrid digital-mixed-signal, computing platform, energy efficiency, pattern formation, reinforcement learning\",\n    \"Supported by Semiconductor Research Corporation under grant JUMP CBRIC task ID 2777.006.\",\n    \"Figures and tables illustrate the system architecture, design components, and energy-scalability.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"40":{"file_id":6947,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"615dab72-8e8b-405f-9fb8-f63e19d6f30f_20250220012423.pdf","file_name":"document.pdf","file_hash":"8288ac0939842570645f2dc909442a0175869cd97a4c755497592aae412dc94e","file_ext":"pdf","upload_date":"2025-02-20T01:24:23.616030","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"7.1 A 3.4-to-13.3TOPS/W 3.6TOPS Dual-Core Deep-Learning Accelerator for Versatile AI Applications in 7nm 5G Smartphone SoC\",\n  \"reference\": \"Lin, C.-H., Cheng, C.-C., Tsai, Y.-M., Hung, S.-J., Kuo, Y.-T., Wang, P. H., Tsung, P.-K., Hsu, J.-Y., Lai, W.-C., Liu, C.-H., Wang, S.-Y., Kuo, C.-H., Chang, C.-Y., Lee, M.-H., Lin, T.-Y., &amp; Chen, C.-C. (2023). A 3.4-to-13.3TOPS/W 3.6TOPS Dual-Core Deep-Learning Accelerator for Versatile AI Applications in 7nm 5G Smartphone SoC, MediaTek, Hsinchu, Taiwan.\",\n  \"doi\": null,\n  \"date\": \"2023\",\n  \"journal\": \"Digest of Technical Papers\",\n  \"authors\": [\n    \"Chien-Hung Lin\",\n    \"Chih-Chung Cheng\",\n    \"Yi-Min Tsai\",\n    \"Sheng-Je Hung\",\n    \"Yu-Ting Kuo\",\n    \"Perry H Wang\",\n    \"Pei-Kuei Tsung\",\n    \"Jeng-Yun Hsu\",\n    \"Wei-Chih Lai\",\n    \"Chia-Hung Liu\",\n    \"Shao-Yu Wang\",\n    \"Chin-Hua Kuo\",\n    \"Chih-Yu Chang\",\n    \"Ming-Hsien Lee\",\n    \"Tsung-Yao Lin\",\n    \"Chih-Cheng Chen\"\n  ],\n  \"context\": \"The article presents a dual-core deep-learning accelerator designed for 7nm 5G smartphone SoCs, focusing on power efficiency and real-time performance for AI applications. It outlines a power-efficient DLA with enhancements to minimize memory access, optimize computational loads, and support various DL operations.\",\n  \"other_metadata\": [\n    \"Keywords: Deep Learning Accelerator, Power Efficiency, Memory Access Optimization\",\n    \"Abstract: The study introduces an efficient DLA with features like ASYMM-Q support, data-reuse techniques, and dual-core execution optimized for smartphone AI applications.\",\n    \"Figures and Tables referenced in article: Figures 7.1.1, 7.1.2, 7.1.3, 7.1.4, 7.1.5, 7.1.6, and 7.1.7.\",\n    \"References included: K. He et al., Y. Chen et al., G. Desoli et al., J. Lee et al., J. Song et al.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"41":{"file_id":6958,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"f3068d14-0884-4b59-8192-3cfeb2d562d9_20250220012435.pdf","file_name":"document.pdf","file_hash":"61dd07fec5693d2e51909de4575cf62825425936387726afdae23e2b311ecb64","file_ext":"pdf","upload_date":"2025-02-20T01:24:35.627594","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices\",\n  \"reference\": \"Chen, Y.-H., Yang, T.-J., Emer, J., & Sze, V. (2019). Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems.\",\n  \"doi\": \"10.1109/JETCAS.2019.2910232\",\n  \"date\": null,\n  \"journal\": \"IEEE Journal on Emerging and Selected Topics in Circuits and Systems\",\n  \"authors\": [\n    \"Yu-Hsin Chen\",\n    \"Tien-Ju Yang\",\n    \"Joel Emer\",\n    \"Vivienne Sze\"\n  ],\n  \"context\": \"The article introduces Eyeriss v2, a DNN accelerator designed for compact and sparse DNNs often used in mobile devices. Eyeriss v2 introduces a novel hierarchical mesh network for efficient data reuse and exploits sparsity to enhance processing speed and energy efficiency. The architecture achieves significant improvements over previous versions, especially with sparse models like MobileNet.\",\n  \"other_metadata\": [\n    \"Keywords: Deep Neural Network Accelerators, Energy-Efficient Accelerators, Dataflow Processing, Spatial Architecture\",\n    \"Abstract: The paper discusses the design of a DNN accelerator architecture, Eyeriss v2, that caters to the needs of compact and sparse DNNs prevalent in mobile devices.\",\n    \"The architecture achieves 12.6x faster throughput and 2.5x more energy efficiency compared to original models.\",\n    \"The work considers the challenges compact and sparse DNN models pose on traditional DNN accelerators.\",\n    \"The design emphasizes accommodating diverse DNN layer shapes and sizes efficiently.\",\n    \"The on-chip hierarchical mesh network enhances computational resource utilization.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"42":{"file_id":6971,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"8a15184c-6b79-432e-8a1f-7348430bce8d_20250220012448.pdf","file_name":"document.pdf","file_hash":"c4228ce6b42f4b285867454423e291f88e45b15fff097497a9d1f3058c6be369","file_ext":"pdf","upload_date":"2025-02-20T01:24:48.228608","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Efficient Processing of Deep Neural Networks: A Tutorial and Survey\",\n  \"reference\": \"Sze, V., Chen, Y.-H., Yang, T.-J., & Emer, J. S. (2017). Efficient Processing of Deep Neural Networks: A Tutorial and Survey. Proceedings of the IEEE, 105(12), 2295-2329. doi:10.1109/JPROC.2017.2761740\",\n  \"doi\": \"10.1109/JPROC.2017.2761740\",\n  \"date\": \"20-11-2017\",\n  \"journal\": \"Proceedings of the IEEE\",\n  \"authors\": [\n    \"Vivienne Sze\",\n    \"Yu-Hsin Chen\",\n    \"Tien-Ju Yang\",\n    \"Joel S. Emer\"\n  ],\n  \"context\": \"This article provides a comprehensive overview and survey of techniques for efficient processing of deep neural networks, focusing on improving energy efficiency and throughput without sacrificing accuracy or increasing hardware costs. It covers DNN architectures and implementation trends, evaluating and comparing different designs.\",\n  \"other_metadata\": [\n    \"Keywords: ASIC, computer architecture, convolutional neural networks, dataflow processing, deep learning, deep neural networks, energy-efficient accelerators, low power, machine learning, spatial architectures, VLSI\",\n    \"Received: 15-03-2017\",\n    \"Revised: 06-08-2017\",\n    \"Accepted: 29-09-2017\",\n    \"Supported by DARPA YFA, MIT CICS, and gifts from Nvidia and Intel\"\n  ]\n}","text":null,"score":null,"raw_url":null},"43":{"file_id":7031,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"3cc27a5e-5bcd-4d6d-9e2b-d73a1d86ddca_20250220012504.pdf","file_name":"document.pdf","file_hash":"0ec25c5d82af592d7a030f125c477f410d26a2e7d62dc7042e65cd2a70b2d23a","file_ext":"pdf","upload_date":"2025-02-20T01:25:04.831193","pipeline_status":"retrying","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":null,"text":null,"score":null,"raw_url":null},"44":{"file_id":7107,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"74024d25-09ec-43da-91e3-dba21a898560_20250220012518.pdf","file_name":"document.pdf","file_hash":"e5eb7ae5b6438b8244d5a10552be56d6a81949cd05967f1bc5bc24a0f4e64306","file_ext":"pdf","upload_date":"2025-02-20T01:25:19.322153","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks\",\n  \"reference\": \"Chen, Y.-H., Krishna, T., Emer, J. S., & Sze, V. (2016). Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. *IEEE Journal of Solid-State Circuits*. https://doi.org/10.1109/JSSC.2016.2616357\",\n  \"doi\": \"10.1109/JSSC.2016.2616357\",\n  \"date\": \"28-09-2016\",\n  \"journal\": \"IEEE Journal of Solid-State Circuits\",\n  \"authors\": [\n    \"Yu-Hsin Chen\",\n    \"Tushar Krishna\",\n    \"Joel S. Emer\",\n    \"Vivienne Sze\"\n  ],\n  \"context\": \"This article presents Eyeriss, a reconfigurable accelerator designed to enhance the energy efficiency of deep convolutional neural networks by minimizing data movement, utilizing row stationary dataflow, and employing techniques like compression and data gating.\",\n  \"other_metadata\": [\n    \"Keywords: Convolutional neural networks, dataflow processing, deep learning, energy-efficient accelerators, spatial architecture.\",\n    \"Article spans various technical aspects including architectural design, energy efficiency features, and benchmarking against standard CNNs like AlexNet and VGG-16.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"45":{"file_id":7163,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"6b8f7cdf-d957-49be-8cae-a8a7e76c5d2d_20250220012533.pdf","file_name":"document.pdf","file_hash":"6cb60e65c18b6e9874f278db4973d8f5b95d0aff714519ba06a01d4a6d3d664b","file_ext":"pdf","upload_date":"2025-02-20T01:25:33.460128","pipeline_status":"pending","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"An Always-On 3.8 \\u03bcJ/86% CIFAR-10 Mixed-Signal Binary CNN Processor With All Memory on Chip in 28-nm CMOS\",\n  \"reference\": \"Bankman, D., Yang, L., Moons, B., Verhelst, M., & Murmann, B. (2018). An Always-On 3.8 \\u00b5J/86% CIFAR-10 Mixed-Signal Binary CNN Processor With All Memory on Chip in 28-nm CMOS. IEEE Journal of Solid-State Circuits, 0018-9200.\",\n  \"doi\": \"10.1109/JSSC.2018.2869150\",\n  \"date\": \"21-08-2018\",\n  \"journal\": \"IEEE Journal of Solid-State Circuits\",\n  \"authors\": [\n    \"Daniel Bankman\",\n    \"Lita Yang\",\n    \"Bert Moons\",\n    \"Marian Verhelst\",\n    \"Boris Murmann\"\n  ],\n  \"context\": \"The paper presents a mixed-signal binary CNN processor achieving 3.8 \\u03bcJ/classification and 86% accuracy on CIFAR-10, focusing on energy efficiency by integrating all memory on-chip and optimizing for always-on applications.\",\n  \"other_metadata\": [\n    \"Keywords: Binarized neural networks, deep learning, mixed-signal processing, near-memory computing, switched-capacitor (SC)\",\n    \"Supported by: MARCO, DARPA through the SONIC program, STARnet Centers\"\n  ]\n}","text":null,"score":null,"raw_url":null},"46":{"file_id":7195,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"fa6cda51-7a18-4ce1-bb68-9021311d4649_20250220012544.pdf","file_name":"document.pdf","file_hash":"4fcfb10f83b3e0ca1b776c173e8eb94ca6ec3127d9bc2f21a3a3e61b631d5b70","file_ext":"pdf","upload_date":"2025-02-20T01:25:44.354325","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"DaDianNao: A Machine-Learning Supercomputer\",\n  \"reference\": \"Chen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun, N., & Temam, O. \\\"DaDianNao: A Machine-Learning Supercomputer.\\\"\",\n  \"doi\": null,\n  \"date\": \"2014\",\n  \"journal\": \"Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture\",\n  \"authors\": [\n    \"Yunji Chen\",\n    \"Tao Luo\",\n    \"Shaoli Liu\",\n    \"Shijin Zhang\",\n    \"Liqiang He\",\n    \"Jia Wang\",\n    \"Ling Li\",\n    \"Tianshi Chen\",\n    \"Zhiwei Xu\",\n    \"Ninghui Sun\",\n    \"Olivier Temam\"\n  ],\n  \"context\": \"The article introduces a custom multi-chip machine-learning architecture designed to enhance performance while reducing energy consumption by leveraging on-chip storage to reduce memory accesses in CNNs/DNNs. On a subset of the largest known neural network layers, the architecture achieves a speedup of 450.65x over a GPU while reducing energy consumption by 150.31x on average for a 64-chip system.\",\n  \"other_metadata\": [\n    \"Field: Computer Architecture, Machine Learning\",\n    \"Keywords: CNNs, DNNs, Hardware acceleration, Neural networks\",\n    \"Objective: To design a specialized architecture that efficiently processes CNNs/DNNs.\",\n    \"Methods: Developed a multi-chip architecture integrating computation and memory storage to minimize memory bottleneck.\",\n    \"Results: Demonstrated significant performance gains and energy reductions compared to GPU usage.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"47":{"file_id":7208,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"a281048b-d4e5-4a8f-ac25-df44abe10320_20250220012554.pdf","file_name":"document.pdf","file_hash":"b16294ce9a96910b932208c0960b445d121d74e407c7794d80eef19178f5c402","file_ext":"pdf","upload_date":"2025-02-20T01:25:54.906139","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Cambricon-X: An Accelerator for Sparse Neural Networks\",\n  \"reference\": \"Zhang, S., Du, Z., Zhang, L., Lan, H., Liu, S., Li, L., Guo, Q., Chen, T., & Chen, Y. (2016). Cambricon-X: An Accelerator for Sparse Neural Networks. *SKL of Computer Architecture, Institute of Computing Technology, CAS, Beijing, China.*\",\n  \"doi\": null,\n  \"date\": \"2016\",\n  \"journal\": \"SKL of Computer Architecture, Institute of Computing Technology, CAS, Beijing, China\",\n  \"authors\": [\n    \"Shijin Zhang\",\n    \"Zidong Du\",\n    \"Lei Zhang\",\n    \"Huiying Lan\",\n    \"Shaoli Liu\",\n    \"Ling Li\",\n    \"Qi Guo\",\n    \"Tianshi Chen\",\n    \"Yunji Chen\"\n  ],\n  \"context\": \"This article proposes Cambricon-X, a novel accelerator designed to efficiently process sparse neural networks, achieving significant speedup and energy reduction compared to existing accelerators. The paper discusses the architectural innovations that allow Cambricon-X to exploit the sparsity and irregularity of neural networks.\",\n  \"other_metadata\": [\n    \"Keywords: Sparse Neural Networks, Accelerator, Processing Elements, Indexing Module, Memory Efficiency\",\n    \"Abstract: Neural networks are intensively used but resource-heavy. Sparse neural networks offer a reduction in computational demand. Cambricon-X leverages network sparsity for efficiency, achieving a 7.23x speedup and 6.43x energy savings compared to state-of-the-art solutions.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"48":{"file_id":7299,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"ecac1978-d698-432b-b2e1-b605b5d662f1_20250220012606.pdf","file_name":"document.pdf","file_hash":"74c4379451948e8aca646bfff77ab6e55739626b0aa7b8ade78ba0cdeacfdf4e","file_ext":"pdf","upload_date":"2025-02-20T01:26:06.660034","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Fused-Layer CNN Accelerators\",\n  \"reference\": \"Alwani, M., Chen, H., Ferdman, M., & Milder, P. (2016). Fused-Layer CNN Accelerators. IEEE.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": \"IEEE\",\n  \"authors\": [\n    \"Manoj Alwani\",\n    \"Han Chen\",\n    \"Michael Ferdman\",\n    \"Peter Milder\"\n  ],\n  \"context\": \"This article explores a novel method for evaluating convolutional neural networks (CNNs) by fusing the processing of multiple layers, thereby reducing off-chip memory transfer. The method was validated using a Xilinx Virtex-7 FPGA, demonstrating a 95% reduction in data transfer for VGGNet-E's first five convolutional layers.\",\n  \"other_metadata\": [\n    \"Keywords: CNN, hardware accelerators, deep learning, FPGA, VGGNet-E\",\n    \"Abstract: The paper focuses on reducing the data transfer between CNN layers by fusing multiple layers' computations, effectively caching intermediate data without off-chip memory usage.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"49":{"file_id":7344,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"0c9c8f50-00d0-4c80-abfa-9b23ed6d5817_20250220012617.pdf","file_name":"document.pdf","file_hash":"3fd65684133dfdfbdb9ac599edec051b1830933675acd050328c932bebb23a14","file_ext":"pdf","upload_date":"2025-02-20T01:26:17.567812","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach\",\n  \"reference\": \"Zhou, X., Du, Z., Guo, Q., Liu, S., Liu, C., Wang, C., Zhou, X., Li, L., Chen, T., & Chen, Y. (2023). Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach. University of Science and Technology of China, Cambricon Technologies Co. Ltd., Institute of Computing Technology, CAS.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Xuda Zhou\",\n    \"Zidong Du\",\n    \"Qi Guo\",\n    \"Shaoli Liu\",\n    \"Chengsi Liu\",\n    \"Chao Wang\",\n    \"Xuehai Zhou\",\n    \"Ling Li\",\n    \"Tianshi Chen\",\n    \"Yunji Chen\"\n  ],\n  \"context\": \"This paper presents Cambricon-S, a software/hardware approach aimed at handling irregularity within sparse neural networks. By introducing coarse-grained pruning and a hardware accelerator, this method significantly reduces irregularity, improving compression and energy efficiency compared to state-of-the-art accelerators.\",\n  \"other_metadata\": [\n    \"Keywords: Sparse Neural Networks, Coarse-grained Pruning, Hardware Accelerator, Local Convergence, Energy Efficiency.\",\n    \"Abstract: The study explores how neural networks' convergence characteristics can be optimized by proposed software techniques and hardware accelerators to manage sparse neural networks.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"50":{"file_id":7386,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"1ccfd9ed-4b92-4db6-a9e1-356c85539e8d_20250220012628.pdf","file_name":"document.pdf","file_hash":"9e2bfaa23e71e703758ed7ab21e349cca348fe1a7f8d73ee37c3aa8313a8114e","file_ext":"pdf","upload_date":"2025-02-20T01:26:28.448701","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Gravitating Towards the Physical Limits of Crossbar Acceleration\",\n  \"reference\": \"Nag, A., Balasubramonian, R., Srikumar, V., Walker, R., Shafiee, A., Strachan, J. P., & Muralimanohar, N. (2023). Gravitating towards the physical limits of crossbar acceleration. University of Utah, Samsung, Hewlett Packard Enterprise, Amazon.\",\n  \"doi\": null,\n  \"date\": \"10-2023\",\n  \"journal\": null,\n  \"authors\": [\n    \"Anirban Nag\",\n    \"Rajeev Balasubramonian\",\n    \"Vivek Srikumar\",\n    \"Ross Walker\",\n    \"Ali Shafiee\",\n    \"John Paul Strachan\",\n    \"Naveen Muralimanohar\"\n  ],\n  \"context\": \"The paper discusses the limitations of analog in-situ computation in crossbar accelerators for deep neural networks and introduces the Newton architecture. It addresses chip power inefficiencies and homogeneous design shortcomings, achieving improved energy efficiency and power reduction compared to existing accelerators.\",\n  \"other_metadata\": [\n    \"Keywords: Crossbar acceleration, analog computation, deep neural networks, neural network inference, energy efficiency.\",\n    \"Abstract: The paper introduces the Newton architecture to improve energy efficiency in memristor crossbar accelerators. Innovative techniques address ADC power consumption and resource provisioning inefficiencies. Results show significant improvements over ISAAC accelerators.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"51":{"file_id":7433,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"e79dee76-d9f7-4733-8638-87b781a0eed9_20250220012638.pdf","file_name":"document.pdf","file_hash":"e3aabf6a795b06b5c7dcc255a6c78bef9d9083716cb7242b76844b2fb0890489","file_ext":"pdf","upload_date":"2025-02-20T01:26:38.792024","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Binarized Neural Networks\",\n  \"reference\": \"Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., & Bengio, Y. (2016). Binarized Neural Networks. In 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\",\n  \"doi\": null,\n  \"date\": \"2016\",\n  \"journal\": \"Proceedings of the 30th Conference on Neural Information Processing Systems\",\n  \"authors\": [\n    \"Itay Hubara\",\n    \"Matthieu Courbariaux\",\n    \"Daniel Soudry\",\n    \"Ran El-Yaniv\",\n    \"Yoshua Bengio\"\n  ],\n  \"context\": \"This paper presents a novel method for training Binarized Neural Networks (BNNs) where both weights and activations are binary. This facilitates significant memory and energy savings due to reduced arithmetic operations. It extends to experiments showing BNNs achieving near state-of-the-art results on several datasets and achieving better efficiency in practical implementations with binary matrix multiplication GPU kernels.\",\n  \"other_metadata\": [\n    \"Keywords: Binarized Neural Networks, AI, deep learning, power-efficiency, memory savings.\",\n    \"Link to code: <sup>1</sup><https://github.com/MatthieuCourbariaux/BinaryNet>\",\n    \"Link to code: <sup>2</sup><https://github.com/itayhubara/BinaryNet>\",\n    \"Presented at NIPS 2016, Barcelona, Spain\",\n    \"Cited techniques: Straight-through estimator, Stochastic Gradient Descent (SGD), Deterministic and Stochastic Binarization.\",\n    \"Experiment frameworks: Torch7, Theano\",\n    \"Datasets used: MNIST, CIFAR-10, SVHN, and preliminary results on ImageNet.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"52":{"file_id":7499,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"0d048607-89ba-43ed-9bcd-2d5221e3385a_20250220012656.pdf","file_name":"document.pdf","file_hash":"cc392d976db72de9f206810802ac7394a44c0c92a7b7951aaa8ee6f5f71bb36c","file_ext":"pdf","upload_date":"2025-02-20T01:26:56.919847","pipeline_status":"pending","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Review and Benchmarking of Precision-Scalable Multiply-Accumulate Unit Architectures for Embedded Neural-Network Processing\",\n  \"reference\": \"Camus, V., Mei, L., Enz, C., & Verhelst, M. (2019). Review and Benchmarking of Precision-Scalable Multiply-Accumulate Unit Architectures for Embedded Neural-Network Processing. *IEEE Journal on Emerging and Selected Topics in Circuits and Systems*, 9(4), 1279-1290.\",\n  \"doi\": \"10.1109/JETCAS.2019.2950386\",\n  \"date\": \"30-10-2019\",\n  \"journal\": \"IEEE Journal on Emerging and Selected Topics in Circuits and Systems\",\n  \"authors\": [\n    \"Vincent Camus\",\n    \"Linyan Mei\",\n    \"Christian Enz\",\n    \"Marian Verhelst\"\n  ],\n  \"context\": \"The article reviews and benchmarks various precision-scalable MAC architectures for embedded neural-network processing, focusing on energy efficiency and computational scalability. It introduces a taxonomy to classify these architectures and benchmarks them under a unified framework.\",\n  \"other_metadata\": [\n    \"Keywords: ASIC, deep neural networks, precision-scalable circuits, configurable circuits, MAC, multiply-accumulate units.\",\n    \"Published: IEEE. Available at: http://ieeexplore.ieee.org\",\n    \"License: Creative Commons Attribution 4.0 License\"\n  ]\n}","text":null,"score":null,"raw_url":null},"53":{"file_id":7567,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"cf686536-6a06-4b1b-961d-324d5f103244_20250220012707.pdf","file_name":"document.pdf","file_hash":"9c331d28639adc7d5b5e9494c443b28bf02dce8b28867dc233487498e7f0cafd","file_ext":"pdf","upload_date":"2025-02-20T01:27:07.927600","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"A Survey of Neural Network Accelerators\",\n  \"reference\": \"Li, Z., Wang, Y., Zhi, T., & Chen, T. (2017). A Survey of Neural Network Accelerators. Higher Education Press and Springer-Verlag Berlin Heidelberg.\",\n  \"doi\": null,\n  \"date\": \"2017\",\n  \"journal\": \"Higher Education Press and Springer-Verlag Berlin Heidelberg\",\n  \"authors\": [\n    \"Zhen Li\",\n    \"Yuqing Wang\",\n    \"Tian Zhi\",\n    \"Tianshi Chen\"\n  ],\n  \"context\": \"This article reviews recent works on neural network accelerators, focusing on the DianNao-family accelerators, highlighting the shift from using energy-inefficient CPUs and GPUs to specialized co-processors for optimal performance and low power consumption in neural network computations.\",\n  \"other_metadata\": [\n    \"Keywords: neural networks, accelerators, FPGAs, ASICs, DianNao series\",\n    \"Abstract: The article outlines the inefficiency of traditional CPUs/GPUs in neural network computations and discusses specialized accelerators designed for enhanced performance and power efficiency.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"54":{"file_id":7588,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"977a7f08-7559-4d72-8af6-2dc986844b7f_20250220012718.pdf","file_name":"document.pdf","file_hash":"e6c5993c1e3bd0cbcb984eba8b705ea83311bb39da2cb92f91b9eacf9d88bed9","file_ext":"pdf","upload_date":"2025-02-20T01:27:18.729127","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"Analytical Guarantees on Numerical Precision of Deep Neural Networks\",\n  \"reference\": \"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\",\n  \"doi\": null,\n  \"date\": \"2017\",\n  \"journal\": \"Proceedings of the 34th International Conference on Machine Learning\",\n  \"authors\": [\n    \"Charbel Sakr\",\n    \"Yongjune Kim\",\n    \"Naresh Shanbhag\"\n  ],\n  \"context\": \"The article presents theoretical bounds on the accuracy of deep neural networks under limited numerical precision. It combines theoretical analysis with the backpropagation algorithm to determine the minimum precision needed while preserving model accuracy. The method is validated on datasets MNIST and CIFAR10, demonstrating reduced complexity.\",\n  \"other_metadata\": [\n    \"Keywords: numerical precision, deep neural networks, fixed-point networks\",\n    \"Abstract: The paper establishes bounds on precision necessary to maintain accuracy and introduces computational and representational cost metrics.\",\n    \"Highlights: Validated on MNIST and CIFAR10, focusing on the interplay of accuracy and precision.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"55":{"file_id":7591,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"401d9564-709f-4019-a23c-af77b78320cb_20250220012729.pdf","file_name":"document.pdf","file_hash":"67b72059ead717a3a11c61e37739345d93994ea85a884bc42fbd410a57677f43","file_ext":"pdf","upload_date":"2025-02-20T01:27:29.976616","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning\",\n  \"reference\": \"Chen, T., Du, Z., Sun, N., Wang, J., Wu, C., Chen, Y., & Temam, O. (2014). \\\"DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning\\\". ACM SIGARCH Computer Architecture News, 42(1), 269\\u2013284. https://doi.org/10.1145/2541940.254____67\",\n  \"doi\": \"10.1145/2541940.2541967\",\n  \"date\": \"01-03-2014\",\n  \"journal\": \"ACM SIGARCH Computer Architecture News\",\n  \"authors\": [\n    \"Tianshi Chen\",\n    \"Zidong Du\",\n    \"Ninghui Sun\",\n    \"Jia Wang\",\n    \"Chengyong Wu\",\n    \"Yunji Chen\",\n    \"Olivier Temam\"\n  ],\n  \"context\": \"The article introduces DianNao, an accelerator designed for large-scale Convolutional and Deep Neural Networks (CNNs and DNNs), targeting high throughput and energy efficiency by minimizing memory transfers.\",\n  \"other_metadata\": [\n    \"Keywords: Convolutional Neural Networks, Deep Neural Networks, Machine Learning Accelerator, Memory Efficiency, High Throughput\",\n    \"Abstract: Machine-Learning tasks are becoming pervasive, and a small set of algorithms like CNNs and DNNs are state-of-the-art across many applications. DianNao achieves 452 GOP/s in a small footprint, opening up the usage of advanced algorithms in varied systems.\",\n    \"Conference: ASPLOS '14, March 1\\u20135, Salt Lake City, Utah, USA\",\n    \"Permissions: Permissions granted for personal or classroom use provided copies are not made for profit. Contact permissions@acm.org for wider distribution.\"\n  ]\n}","text":null,"score":null,"raw_url":null},"56":{"file_id":7677,"parent_file_id":null,"direct_parent_file_id":null,"file_uuid":"8a6634dd-6350-4cc9-baf5-df8689378cbc_20250220012740.pdf","file_name":"document.pdf","file_hash":"ecf914e1a9bba59c4ee45197cce86d8026e74652eae4c52e876f205f1eca4daf","file_ext":"pdf","upload_date":"2025-02-20T01:27:40.444586","pipeline_status":"success","ext_project_id":"c-f3d4b8d6-4a1b-4f0f-9b2d-0c2f0b7f0d4e","context":null,"position":null,"description":"{\n  \"title\": \"SNAP: A 1.67 - 21.55TOPS/W Sparse Neural Acceleration Processor for Unstructured Sparse Deep Neural Network Inference in 16nm CMOS\",\n  \"reference\": \"Zhang, J. F., Lee, C. E., Liu, C., Shao, Y. S., Keckler, S. W., & Zhang, Z. (n.d.). SNAP: A 1.67 - 21.55TOPS/W Sparse Neural Acceleration Processor for Unstructured Sparse Deep Neural Network Inference in 16nm CMOS.\",\n  \"doi\": null,\n  \"date\": null,\n  \"journal\": null,\n  \"authors\": [\n    \"Jie-Fang Zhang\",\n    \"Ching-En Lee\",\n    \"Chester Liu\",\n    \"Yakun Sophia Shao\",\n    \"Stephen W. Keckler\",\n    \"Zhengya Zhang\"\n  ],\n  \"context\": \"The article presents SNAP, a Sparse Neural Acceleration Processor aimed at exploiting unstructured sparsity in deep neural networks. It tackles challenges of hardware utilization and data traffic through innovative methodologies like associative index matching and two-level partial sum reduce, achieving notable energy efficiency and throughput in sparse DNN inference.\",\n  \"other_metadata\": [\n    \"Keywords: Sparse Neural Acceleration, Deep Neural Networks, Hardware Utilization, Associative Index Matching\",\n    \"Abstract: SNAP achieves high efficiency in DNN inference by addressing hardware underutilization and data contention challenges using sparse data methodologies.\",\n    \"Acknowledgements: Supported by Toyota Research Institute and DARPA.\"\n  ]\n}","text":null,"score":null,"raw_url":null}},"uuid_2_position":{"7ee1459a-47ce-4380-acde-a911eceddbed_20250220011643.pdf":0,"ee491b90-eb6b-4d29-962b-18f0259d860a_20250220011658.pdf":1,"0282f4a2-7cfa-4912-820b-83386f2e6824_20250220011710.pdf":2,"d7d270fb-f7f5-4dfe-9680-d15b0c291f5a_20250220011723.pdf":3,"33a6208d-1427-4258-aa74-08715d3180af_20250220011735.pdf":4,"6a65efc7-fcc8-459a-98ba-fc424f28081f_20250220011745.pdf":5,"cb4b0d2d-b828-4b13-8a02-f9bb0d3034d8_20250220011756.pdf":6,"7418cd95-d90a-429b-96a2-9564f806ed85_20250220011807.pdf":7,"1b116ceb-d7ac-44b9-b4a7-55bc485f41bc_20250220011818.pdf":8,"c2b18efc-b49d-4e97-b35d-53d055bf6936_20250220011830.pdf":9,"84d15e2a-9452-467b-9fb8-8beadb598c42_20250220011842.pdf":10,"3f086a35-5e50-4690-915e-98d421fdfc61_20250220011852.pdf":11,"b1aabb1d-fd76-4291-918f-c3b50fbb7c3c_20250220011903.pdf":12,"46c0fe25-b89e-4903-aa9c-c6d83a24f508_20250220011915.pdf":13,"a65cb288-d6ca-44ec-8824-e0a150d3cc7f_20250220011925.pdf":14,"073592e9-3c31-48f1-8890-7885c0372543_20250220011937.pdf":15,"96950e88-0dbd-4409-9aeb-9f2dca45522b_20250220011949.pdf":16,"8f869cd1-b2c5-4315-99ba-ae94d6abcbbd_20250220012000.pdf":17,"79ca761b-b063-4069-bd9b-b2f6656403cb_20250220012011.pdf":18,"c7f46b29-5d07-4d0e-a2b1-6efbde996a96_20250220012027.pdf":19,"af0d8596-f143-4b1c-ac47-737c0d28018a_20250220012038.pdf":20,"88d696ee-ab83-4e17-a896-6a9e79e928ec_20250220012049.pdf":21,"062209ac-baa6-45da-b320-ffd67d9377e7_20250220012059.pdf":22,"96482662-7be2-490e-984e-30970b6710d0_20250220012110.pdf":23,"8f0f2741-1de4-4d31-81eb-731730fe63dc_20250220012121.pdf":24,"4210e6b4-321d-44da-9797-a27ed5e2cb87_20250220012132.pdf":25,"5698d43a-a7f9-44a7-aa19-c107c8dfddad_20250220012143.pdf":26,"11c9ee84-d1e7-4173-a805-07bd1ebc7c62_20250220012154.pdf":27,"96fe7b08-e8c0-4888-851a-28afb7dbd150_20250220012205.pdf":28,"515a8b27-07f3-4805-82f9-12412722960c_20250220012216.pdf":29,"a80092ab-79e1-4993-b6ec-305bce97d18c_20250220012228.pdf":30,"f8a19085-a79a-49d9-b7ef-29fe9ea2242f_20250220012239.pdf":31,"b4b25d67-c7ce-413a-9136-2db2d40e8b14_20250220012251.pdf":32,"c21f5716-ce1f-4335-959b-2b580e1f2cc7_20250220012301.pdf":33,"52e12a77-f4f2-4f0f-bd29-e94f917242cc_20250220012313.pdf":34,"e1af5084-4972-4f6e-9038-43ab10e31e2f_20250220012325.pdf":35,"7c90cc1d-9f4e-4ef5-9110-facec8a05f58_20250220012339.pdf":36,"60a496ca-bd46-4dcc-832f-649f0455792b_20250220012351.pdf":37,"ff6b5d1d-c427-43e5-a0c8-37fe7fbc3658_20250220012402.pdf":38,"06351ded-b0cd-46fb-a659-af27084bee05_20250220012412.pdf":39,"615dab72-8e8b-405f-9fb8-f63e19d6f30f_20250220012423.pdf":40,"f3068d14-0884-4b59-8192-3cfeb2d562d9_20250220012435.pdf":41,"8a15184c-6b79-432e-8a1f-7348430bce8d_20250220012448.pdf":42,"3cc27a5e-5bcd-4d6d-9e2b-d73a1d86ddca_20250220012504.pdf":43,"74024d25-09ec-43da-91e3-dba21a898560_20250220012518.pdf":44,"6b8f7cdf-d957-49be-8cae-a8a7e76c5d2d_20250220012533.pdf":45,"fa6cda51-7a18-4ce1-bb68-9021311d4649_20250220012544.pdf":46,"a281048b-d4e5-4a8f-ac25-df44abe10320_20250220012554.pdf":47,"ecac1978-d698-432b-b2e1-b605b5d662f1_20250220012606.pdf":48,"0c9c8f50-00d0-4c80-abfa-9b23ed6d5817_20250220012617.pdf":49,"1ccfd9ed-4b92-4db6-a9e1-356c85539e8d_20250220012628.pdf":50,"e79dee76-d9f7-4733-8638-87b781a0eed9_20250220012638.pdf":51,"0d048607-89ba-43ed-9bcd-2d5221e3385a_20250220012656.pdf":52,"cf686536-6a06-4b1b-961d-324d5f103244_20250220012707.pdf":53,"977a7f08-7559-4d72-8af6-2dc986844b7f_20250220012718.pdf":54,"401d9564-709f-4019-a23c-af77b78320cb_20250220012729.pdf":55,"8a6634dd-6350-4cc9-baf5-df8689378cbc_20250220012740.pdf":56}}